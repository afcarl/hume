{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hume\n",
    "\n",
    "__Hume__ is a tool for building, deploying and interacting with machine learning models. Hume tries to solve the problem of creating machine learning artifacts that are completely portable wihout having to peg requirement versions. Hume also provides a framework for machine learning IO which we believe is more composable.\n",
    "\n",
    "In this notebook we'll walk through building your first __hume__ artefact using an example machine learning problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): hume in /usr/local/lib/python2.7/site-packages\n",
      "Requirement already satisfied (use --upgrade to upgrade): docopt in /usr/local/lib/python2.7/site-packages (from hume)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "PIP_EXTRA_INDEX_URL=http://54.83.192.85:8080\n",
    "pip install hume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before starting, you'll need to have Docker __1.10.0__ installed or greater. You can check with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client:\r\n",
      " Version:      1.10.2\r\n",
      " API version:  1.22\r\n",
      " Go version:   go1.5.3\r\n",
      " Git commit:   c3959b1\r\n",
      " Built:        Mon Feb 22 22:37:33 2016\r\n",
      " OS/Arch:      darwin/amd64\r\n",
      "\r\n",
      "Server:\r\n",
      " Version:      1.10.2\r\n",
      " API version:  1.22\r\n",
      " Go version:   go1.5.3\r\n",
      " Git commit:   c3959b1\r\n",
      " Built:        Mon Feb 22 22:37:33 2016\r\n",
      " OS/Arch:      linux/amd64\r\n"
     ]
    }
   ],
   "source": [
    "! docker version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, make sure you have a docker-machine instance running if you're not running on Linux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll be using `pandashells` for doing some quick command line transformations, so make sure you have it installed. You can do so by running the following "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1692,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): pandashells in /usr/local/lib/python2.7/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "! pip install pandashells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /tmp/humedemo: File exists\n",
      "/private/tmp/humedemo\n"
     ]
    }
   ],
   "source": [
    "!mkdir /tmp/humedemo\n",
    "%cd /tmp/humedemo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "First let's download the data we'll be using. For this example we'll be using the __iris__ dataset from the Rdatasets set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1252,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-03-16 16:09:30--  https://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv\n",
      "Resolving vincentarelbundock.github.io... 23.235.44.133\n",
      "Connecting to vincentarelbundock.github.io|23.235.44.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4821 (4.7K) [text/csv]\n",
      "Saving to: 'iris.csv.2'\n",
      "\n",
      "iris.csv.2          100%[===================>]   4.71K  --.-KB/s    in 0s      \n",
      "\n",
      "2016-03-16 16:09:33 (270 MB/s) - 'iris.csv.2' saved [4821/4821]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id\",\"Sepal.Length\",\"Sepal.Width\",\"Petal.Length\",\"Petal.Width\",\"Species\"\r\n",
      "\"1\",5.1,3.5,1.4,0.2,\"setosa\"\r\n",
      "\"2\",4.9,3,1.4,0.2,\"setosa\"\r\n",
      "\"3\",4.7,3.2,1.3,0.2,\"setosa\"\r\n",
      "\"4\",4.6,3.1,1.5,0.2,\"setosa\"\r\n",
      "\"5\",5,3.6,1.4,0.2,\"setosa\"\r\n",
      "\"6\",5.4,3.9,1.7,0.4,\"setosa\"\r\n",
      "\"7\",4.6,3.4,1.4,0.3,\"setosa\"\r\n",
      "\"8\",5,3.4,1.5,0.2,\"setosa\"\r\n",
      "\"9\",4.4,2.9,1.4,0.2,\"setosa\"\r\n"
     ]
    }
   ],
   "source": [
    "# Make sure you have it and do some cleaning\n",
    "!sed -i .bak '1,2s/\\\"\\\"/\"id\"/' iris.csv && rm iris.csv.bak && head iris.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1: K-Means\n",
    "For this example we'll be training a k-means model on the iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model\n",
    "First let's write out our estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1507,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile demo.py\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fit(X, params):\n",
    "    \"Fit a model using data with the given parameters\"\n",
    "    kmeans = KMeans(**params)\n",
    "    kmeans.fit(X)\n",
    "    # As a side effect, we'll save the model so that we can predict\n",
    "    # with it later.\n",
    "    with open(\"./fitted.pkl\", \"w\") as f:\n",
    "        pickle.dump(kmeans, f)\n",
    "    return kmeans\n",
    "\n",
    "def predict(X):\n",
    "    \"Predict cluster membership\"\n",
    "    try:\n",
    "        with open(\"./fitted.pkl\", \"rb\") as f:\n",
    "            kmeans = pickle.load(f)\n",
    "    except IOError as e:\n",
    "        raise IOError(\"No fitted model found at {}. Train your model first before calling predict.\".format(e.filename))\n",
    "    else:\n",
    "        return kmeans.predict(X)\n",
    "\n",
    "def split_X_y(df, target):\n",
    "    \"Prep the data for modeling\"\n",
    "    X, y = df.drop(target, axis=1), df[target]\n",
    "    return X, y\n",
    "\n",
    "def fit_main(args):\n",
    "\n",
    "    params_file  = os.getenv(\"HUME_PARAMS_FILE\")\n",
    "    data_file    = os.getenv(\"HUME_TRAIN_DATA\")\n",
    "    target_label = os.getenv(\"HUME_TARGET_LABEL\")\n",
    "    \n",
    "    with open(params_file, \"rb\") as f:\n",
    "        params = json.load(f)\n",
    "    df = pd.read_csv(data_file, header=None)\n",
    "    X, y = split_X_y(df.pivot(0,1,2), target_label)\n",
    "    print(\"params: \"+str(params))\n",
    "    print(\"params type: \"+str(type(params)))\n",
    "    print(fit(X, params))\n",
    "    \n",
    "def predict_main(args):\n",
    "    # print(\"args.sample = {}\".format(args.sample))\n",
    "    X = pd.read_csv(args.sample, header=None)\n",
    "    out = predict(X)\n",
    "    np.savetxt(sys.stdout, out, delimiter=\",\")\n",
    "\n",
    "def make_parser():\n",
    "    # create the top-level parser\n",
    "    parser = argparse.ArgumentParser(prog='Hume inter')\n",
    "    subparsers = parser.add_subparsers()\n",
    "\n",
    "    # create the parser for the \"fit\" command\n",
    "    parser_fit = subparsers.add_parser('fit')\n",
    "    parser_fit.set_defaults(func=fit_main)\n",
    "\n",
    "    # create the parser for the \"predict\" command\n",
    "    parser_predict = subparsers.add_parser('predict')\n",
    "    parser_predict.add_argument('sample', nargs=\"?\", type=argparse.FileType('r'), default=sys.stdin)\n",
    "    parser_predict.set_defaults(func=predict_main)\n",
    "    return parser\n",
    "\n",
    "# A trick to make sure we're not being called from an ipython notebook\n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():\n",
    "    parser = make_parser()\n",
    "    args = parser.parse_args()\n",
    "    args.func(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Environment\n",
    "Now we need a Docker as the runtime environment for our model. For simplicity, we'll just inherit from a pre-built scipy image since it has all the dependencies we'll need to run our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1508,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile Dockerfile\n",
    "FROM registry.spartzinc.com:5000/dose_scipy27:latest\n",
    "MAINTAINER Troy de Freitas troy@dose.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the image does not come with `sklearn`, so we need to add a directive to install it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1509,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a Dockerfile\n",
    "\n",
    "RUN pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to tell Docker where to put our model and how to run it. The `ENTRYPOINT` is significant to Hume. Hume parses the Dockerfile looking for the `ENTRYPOINT` as the gateway for running commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1510,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a Dockerfile\n",
    "\n",
    "COPY . /opt/model\n",
    "\n",
    "ENTRYPOINT [\"python\", \"/opt/model/demo.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In future versions you'll be able to indicate the comamnds for different operations using environment variables, like for instance\n",
    "```docker\n",
    "ENV HUME_CMD_FIT \"python /opt/model/demo.py --fit\"\n",
    "ENV HUME_CMD_PREDICT \"python /opt/model/demo.py --predict\"\n",
    "ENV HUME_CMD_PARAMS \"cat /opt/hume/params\"\n",
    "```\n",
    "but we'll just stick with the entrypoint for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, our Dockerfile looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1511,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FROM registry.spartzinc.com:5000/dose_scipy27:latest\r\n",
      "MAINTAINER Troy de Freitas troy@dose.com\r\n",
      "RUN pip install sklearn\r\n",
      "COPY . /opt/model\r\n",
      "\r\n",
      "ENTRYPOINT [\"python\", \"/opt/model/demo.py\"]"
     ]
    }
   ],
   "source": [
    "!cat Dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've written a Dockerfile specifying our estimator, we're ready to build our Hume container. To build your container, simply run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1512,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hume build demo . &1>/dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hume just built a Docker image tagged as `demo` with the Dockefile in our current directory. Once we've built our estimator, we now have a reusable artefact that we can retrain multiple times with different data and different parameters while reusing only one base image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "Often times you want to train a model with the same estimator but with different parameters. Hume supports passing parameters to estimators in a couple of ways: via a `params.yml` file in the same directory as your Dockerfile or via command line arguments. We'll use the first method for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1513,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting params.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile params.yml\n",
    "n_clusters: 4\n",
    "init: \"k-means++\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning data customarily comes in a matrix or wide format. However Hume takes data in a long format which is better for encoding sparse data and for streaming. We'll do a quick transformation using Pandas to munge our iris dataset into a long format which Hume accepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1514,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"id\",\"variable\",\"value\"\n",
      "1,\"Sepal.Length\",5.1\n",
      "2,\"Sepal.Length\",4.9\n",
      "3,\"Sepal.Length\",4.7\n",
      "4,\"Sepal.Length\",4.6\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cat iris.csv | p.df \"pd.melt(df, id_vars='id')\" > iris_long.csv\n",
    "head -n 5 iris_long.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Fitting\n",
    "\n",
    "Now that we have our parameters and data in the right format, we're ready to fit our model. We'll pass the long-style csv we created to Hume via stdin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1515,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating build context for Docker image at /var/folders/6q/_wph_y897bgf0q5xss3jltpw0000gn/T/tmpDzZ6qz\n",
      "Sending build context to Docker daemon 19.46 kB\n",
      "Step 1 : FROM demo\n",
      "\u001b[91m# Executing 13 build triggers...\n",
      "\u001b[0mStep 1 : RUN mkdir /opt/hume\n",
      " ---> Running in 6e9987bd4101\n",
      "Step 1 : ARG TRAIN_WITH\n",
      " ---> Running in e52c183fea4b\n",
      "Step 1 : ENV TRAIN_WITH ${TRAIN_WITH}\n",
      " ---> Running in 15f9632f5197\n",
      "Step 1 : ENV HUME_TRAIN_DATA /opt/hume/data\n",
      " ---> Running in 09eca22fecac\n",
      "Step 1 : COPY ${TRAIN_WITH} $HUME_TRAIN_DATA\n",
      "Step 1 : ARG TRAIN_PARAMS=\n",
      " ---> Running in 81932298222f\n",
      "Step 1 : ENV TRAIN_PARAMS \"${TRAIN_PARAMS}\"\n",
      " ---> Running in 78d17834bfca\n",
      "Step 1 : ENV HUME_PARAMS_FILE /opt/hume/params\n",
      " ---> Running in bbe81ea5f763\n",
      "Step 1 : RUN echo $TRAIN_PARAMS\n",
      " ---> Running in 1b172664fe74\n",
      "{\"init\": \"k-means++\", \"n_clusters\": 4}\n",
      "Step 1 : RUN echo $TRAIN_PARAMS > $HUME_PARAMS_FILE\n",
      " ---> Running in 9c18b87fcf63\n",
      "Step 1 : ARG TARGET_LABEL=target\n",
      " ---> Running in 16dbc6a24e38\n",
      "Step 1 : ENV HUME_TARGET_LABEL ${TARGET_LABEL}\n",
      " ---> Running in d8713ac98c66\n",
      "Step 1 : RUN python /opt/model/demo.py fit\n",
      " ---> Running in a4c1116f43ff\n",
      "params: {u'init': u'k-means++', u'n_clusters': 4}\n",
      "params type: <type 'dict'>\n",
      "KMeans(copy_x=True, init=u'k-means++', max_iter=300, n_clusters=4, n_init=10,\n",
      "    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,\n",
      "    verbose=0)\n",
      " ---> da94b293fbdb\n",
      "Removing intermediate container bbe81ea5f763\n",
      "Removing intermediate container 9c18b87fcf63\n",
      "Removing intermediate container a4c1116f43ff\n",
      "Removing intermediate container 6e9987bd4101\n",
      "Removing intermediate container e52c183fea4b\n",
      "Removing intermediate container 75d6f2682e25\n",
      "Removing intermediate container 81932298222f\n",
      "Removing intermediate container 16dbc6a24e38\n",
      "Removing intermediate container d8713ac98c66\n",
      "Removing intermediate container 15f9632f5197\n",
      "Removing intermediate container 09eca22fecac\n",
      "Removing intermediate container 78d17834bfca\n",
      "Removing intermediate container 1b172664fe74\n",
      "Successfully built da94b293fbdb\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note that the `-` tells Hume to accept data via stdin\n",
    "!tail -n+2 iris_long.csv | hume fit --target Species demo -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running `hume fit [..]` caused hume to take our original `demo` image along with the parameters specified in `params.yml` and generated a new image tagged as `demo:fitted` which has the same environment as the original, but now with a fitted model. Once we have a `:fitted` version of an image we can use it to generate predictions from samples.\n",
    "\n",
    "We can score single observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1516,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000000000000000e+00\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "! echo \"0, 1, 2, 3\" | hume predict demo:fitted -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or score in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1517,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000000000000000e+00\n",
      "3.000000000000000000e+00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "hume predict demo:fitted - <<EOF \n",
    "0, 1, 2, 3\n",
    "4, 5, 8, 9\n",
    "EOF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting a Model\n",
    "Hume allows you to inspect models you've created ex-post-facto with a few convenience commands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting and Setting Parameters\n",
    "\n",
    "Suppose we trained a model but we don't remember what parameters were used to fit it. To recall the fitting parameters, we can just call "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1518,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "    \"init\": \"k-means++\", \r\n",
      "    \"n_clusters\": 4\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! hume params demo:fitted "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note__: In future versions hume will allow you to get and set parameters inplace via the commandline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Your Own Hume Containers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our first example, we kind of glossed over the details of how to create a model that Hume can use. Now we'll walk through the steps needed to write a model that targets Hume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the iris dataset, let's now write an SVM model that Hume can build."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hume exports a number of environment variables that we can access while within our model:\n",
    "- `HUME_TRAIN_DATA`: the path to the training data.\n",
    "- `HUME_PARAMS_FILE`: the path to the parameters file (stored as json).\n",
    "- `HUME_TARGET_LABEL`: the name of the target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1683,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ex1/demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ex1/demo.py\n",
    "import sys, os, json, pickle\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "TRAINING_DATA = os.getenv(\"HUME_TRAIN_DATA\")\n",
    "TARGET_LABEL = os.getenv(\"HUME_TARGET_LABEL\")\n",
    "with open(os.getenv(\"HUME_PARAMS_FILE\")) as f:\n",
    "    SVM_PARAMS = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use these to source the data and separate the features matrix from the target vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1684,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def split_X_y(df):\n",
    "    X, y = df.drop(TARGET_LABEL, axis=1), df[TARGET_LABEL]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fit method is significant because its the only time we get to save data to persist in the container before a fitted version is built. In general, you should only save data that you need for scoring, like a serialized fitted model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1685,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fit():\n",
    "    svm = SVC(**SVM_PARAMS)\n",
    "    df = pd.read_csv(TRAINING_DATA)\n",
    "    # Since we're piping observations in long format, we'll need to pivot the data\n",
    "    # befor splitting. This however is not a strict Hume requirement -- you can do\n",
    "    # whatever suits your usecase so long as it reads a csv from stdin and writes\n",
    "    # a csv to stdout.\n",
    "    X, y = split_X_y(df.pivot(\"id\",\"variable\",\"value\"))\n",
    "    svm.fit(X,y)\n",
    "    with open(\"/opt/model/fitted.pkl\", \"wb\") as f:\n",
    "        pickle.dump(svm, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can source the data we've saved in our predict method.\n",
    "Note that predict receives its data via stdin and reports its scores via stdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1686,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict():\n",
    "    with open(\"/opt/model/fitted.pkl\", \"rb\") as f:\n",
    "        svm = pickle.load(f)\n",
    "    data = pd.read_csv(sys.stdin)\n",
    "    preds = svm.predict(data.pivot(\"id\",\"variable\",\"value\"))\n",
    "    pd.Series(preds).to_csv(sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll write a main method that dispatches calls to their respective methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1687,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    args = sys.argv[1:]\n",
    "    method = args[0]\n",
    "    if method == \"fit\":\n",
    "        fit()\n",
    "    elif method == \"predict\":\n",
    "        predict()\n",
    "    else:\n",
    "        raise NotImplementedError(\"Command {} is not implemented\".format(method))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1690,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ex1/demo.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ex1/demo.py\n",
    "import sys, os, json, pickle\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "TRAINING_DATA = os.getenv(\"HUME_TRAIN_DATA\")\n",
    "TARGET_LABEL = os.getenv(\"HUME_TARGET_LABEL\")\n",
    "with open(os.getenv(\"HUME_PARAMS_FILE\")) as f:\n",
    "    SVM_PARAMS = json.load(f)\n",
    "    \n",
    "def split_X_y(df):\n",
    "    X, y = df.drop(TARGET_LABEL, axis=1), df[TARGET_LABEL]\n",
    "    return X, y\n",
    "\n",
    "def fit():\n",
    "    svm = SVC(**SVM_PARAMS)\n",
    "    df = pd.read_csv(TRAINING_DATA)\n",
    "    X, y = split_X_y(df.pivot(\"id\",\"variable\",\"value\"))\n",
    "    svm.fit(X,y)\n",
    "    with open(\"/opt/model/fitted.pkl\", \"wb\") as f:\n",
    "        pickle.dump(svm, f)\n",
    "\n",
    "def predict():\n",
    "    with open(\"/opt/model/fitted.pkl\", \"rb\") as f:\n",
    "        svm = pickle.load(f)\n",
    "    data = pd.read_csv(sys.stdin)\n",
    "    preds = svm.predict(data.pivot(\"id\",\"variable\",\"value\"))\n",
    "    pd.Series(preds).to_csv(sys.stdout)\n",
    "\n",
    "def main():\n",
    "    args = sys.argv[1:]\n",
    "    method = args[0]\n",
    "    if method == \"fit\":\n",
    "        fit()\n",
    "    elif method == \"predict\":\n",
    "        predict()\n",
    "    else:\n",
    "        raise NotImplementedError(\"Command {} is not implemented\".format(method))\n",
    "    \n",
    "\n",
    "if __name__ == '__main__' and \"get_ipython\" not in dir():\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also write out our parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1691,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ex1/params.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile ex1/params.yml\n",
    "probability: True\n",
    "tol: 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1633,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hume build svm ./ex1 &1>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1586,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating build context for Docker image at /var/folders/6q/_wph_y897bgf0q5xss3jltpw0000gn/T/tmpXK37Xz\n",
      "Sending build context to Docker daemon 19.46 kB\n",
      "Step 1 : FROM svm\n",
      "\u001b[91m# Executing 13 build triggers...\n",
      "\u001b[0mStep 1 : RUN mkdir /opt/hume\n",
      " ---> Running in 70cd9b190fde\n",
      "Step 1 : ARG TRAIN_WITH\n",
      " ---> Running in d2073598c88b\n",
      "Step 1 : ENV TRAIN_WITH ${TRAIN_WITH}\n",
      " ---> Running in 60bc1aa3ce0c\n",
      "Step 1 : ENV HUME_TRAIN_DATA /opt/hume/data\n",
      " ---> Running in 0f592e1da607\n",
      "Step 1 : COPY ${TRAIN_WITH} $HUME_TRAIN_DATA\n",
      "Step 1 : ARG TRAIN_PARAMS=\n",
      " ---> Running in 5e5108deadac\n",
      "Step 1 : ENV TRAIN_PARAMS \"${TRAIN_PARAMS}\"\n",
      " ---> Running in 4d1822221cb7\n",
      "Step 1 : ENV HUME_PARAMS_FILE /opt/hume/params\n",
      " ---> Running in 31929f2cc22c\n",
      "Step 1 : RUN echo $TRAIN_PARAMS\n",
      " ---> Running in 7e4d70bc0efa\n",
      "{\"probability\": true, \"tol\": 0.001}\n",
      "Step 1 : RUN echo $TRAIN_PARAMS > $HUME_PARAMS_FILE\n",
      " ---> Running in b2ac94122608\n",
      "Step 1 : ARG TARGET_LABEL=target\n",
      " ---> Running in ed7b6408f0d7\n",
      "Step 1 : ENV HUME_TARGET_LABEL ${TARGET_LABEL}\n",
      " ---> Running in dc6a958e05cb\n",
      "Step 1 : RUN python /opt/model/demo.py fit\n",
      " ---> Running in 5e9200663ba4\n",
      " ---> cb7cb3536ef9\n",
      "Removing intermediate container 5e9200663ba4\n",
      "Removing intermediate container 70cd9b190fde\n",
      "Removing intermediate container d2073598c88b\n",
      "Removing intermediate container 60bc1aa3ce0c\n",
      "Removing intermediate container 0f592e1da607\n",
      "Removing intermediate container 63ada141cb37\n",
      "Removing intermediate container 5e5108deadac\n",
      "Removing intermediate container ed7b6408f0d7\n",
      "Removing intermediate container 4d1822221cb7\n",
      "Removing intermediate container 31929f2cc22c\n",
      "Removing intermediate container 7e4d70bc0efa\n",
      "Removing intermediate container b2ac94122608\n",
      "Removing intermediate container dc6a958e05cb\n",
      "Successfully built cb7cb3536ef9\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cd ex1 && hume fit --target Species svm - < ../iris_long.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1632,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0,setosa\n",
      "1,setosa\n",
      "2,setosa\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bash \n",
    "grep -v \"Species\" iris_long.csv | # Remove the target variable\n",
    "awk -F ',' '($1 <= 3){print}'   | # Use only a subset of lines (where id<=3)\n",
    "hume predict svm:fitted -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hume as a SciKit Learn Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the sklearn adapter we can use Hume as if it were any Sklearn estimator. Since it obeys the same interface we can use Hume containers for grid search and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from hume.ext.sklearn import SciKitHume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1347,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skhume = SciKitHume(\"demo\", params={\"init\": \"k-means++\", \"n_clusters\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1348,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_data = pd.read_csv(\"./iris.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1349,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = iris_data['Species']\n",
    "X = iris_data.drop('Species', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "skhume.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1351,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,\n",
       "        3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  3.,  2.,  2.,\n",
       "        2.,  3.,  2.,  0.,  2.,  3.,  2.,  3.,  3.,  0.,  3.,  0.,  3.,\n",
       "        2.,  0.,  0.,  0.,  3.,  0.,  0.,  0.,  0.,  0.,  2.,  2.,  2.,\n",
       "        0.,  3.,  3.,  3.,  3.,  0.,  0.,  0.,  2.,  0.,  0.,  3.,  0.,\n",
       "        0.,  3.,  3.,  0.,  0.,  0.,  0.,  3.,  0.,  2.,  0.,  2.,  2.,\n",
       "        2.,  2.,  3.,  2.,  2.,  2.,  2.,  2.,  2.,  0.,  0.,  2.,  2.,\n",
       "        2.,  2.,  0.,  2.,  0.,  2.,  2.,  2.,  2.,  0.,  0.,  2.,  2.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  2.,  2.,  0.,  2.,  2.,  2.,  0.,\n",
       "        2.,  2.,  2.,  2.,  2.,  2.,  0.])"
      ]
     },
     "execution_count": 1351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skhume.predict(X.drop(\"target\", axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
